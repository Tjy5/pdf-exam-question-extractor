#!/usr/bin/env python3
"""OCR æ€§èƒ½æµ‹è¯•è„šæœ¬ - å¢å¼ºç‰ˆï¼ˆå¸¦è¯¦ç»†æ€§èƒ½æ—¥å¿—ï¼‰"""
import os
import sys
import time
import json
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡ - å¼€å¯æ€§èƒ½è¿½è¸ª
os.environ.setdefault("EXAMPAPER_USE_GPU", "1")
os.environ.setdefault("EXAMPAPER_PARALLEL_EXTRACTION", "1")
os.environ.setdefault("EXAMPAPER_DET_BATCH_SIZE", "4")
os.environ.setdefault("EXAMPAPER_REC_BATCH_SIZE", "32")
os.environ.setdefault("EXAMPAPER_PREFETCH_SIZE", "8")
os.environ.setdefault("EXAMPAPER_MAX_WORKERS", "2")
os.environ.setdefault("EXAMPAPER_GPU_CONCURRENCY", "1")

# ğŸ”¥ å…³é”®ï¼šå¼€å¯æ€§èƒ½æ—¥å¿—
os.environ["EXAMPAPER_PERF_LOG"] = "1"
os.environ["EXAMPAPER_PERF_TRACE"] = "perf_trace.jsonl"

from backend.src.common.ocr_models import get_ppstructure, warmup_ppstructure
from backend.src.services.parallel_extraction import ParallelPageProcessor
from backend.src.services.pipeline.impl.extract_questions import (
    extract_questions_from_page,
    save_questions_for_page,
)


def is_valid_meta(meta_path: Path) -> bool:
    """æ£€æŸ¥ meta.json æ˜¯å¦æœ‰æ•ˆ"""
    return meta_path.exists() and meta_path.stat().st_size > 10


def main():
    # æµ‹è¯•å›¾ç‰‡è·¯å¾„
    test_dir = Path("pdf_images/æµ‹è¯•")
    img_paths = sorted(test_dir.glob("*.png"))

    if not img_paths:
        print("[ERROR] æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡")
        sys.exit(1)

    print("=" * 60)
    print("  OCR æ€§èƒ½æµ‹è¯• (DEBUG æ¨¡å¼)")
    print("=" * 60)
    print(f"  æµ‹è¯•å›¾ç‰‡: {len(img_paths)} å¼ ")
    print(f"  DET_BATCH_SIZE: {os.getenv('EXAMPAPER_DET_BATCH_SIZE')}")
    print(f"  REC_BATCH_SIZE: {os.getenv('EXAMPAPER_REC_BATCH_SIZE')}")
    print(f"  PREFETCH_SIZE: {os.getenv('EXAMPAPER_PREFETCH_SIZE')}")
    print(f"  MAX_WORKERS: {os.getenv('EXAMPAPER_MAX_WORKERS')}")
    print(f"  GPU_CONCURRENCY: {os.getenv('EXAMPAPER_GPU_CONCURRENCY')}")
    print(f"  æ€§èƒ½è¿½è¸ª: {os.getenv('EXAMPAPER_PERF_TRACE')}")
    print("=" * 60)

    # åˆå§‹åŒ–æ¨¡å‹
    print("\n[1/3] åˆå§‹åŒ– PP-StructureV3 æ¨¡å‹...")
    t0 = time.perf_counter()
    pipeline = get_ppstructure()
    init_time = time.perf_counter() - t0
    print(f"  æ¨¡å‹åˆå§‹åŒ–è€—æ—¶: {init_time:.2f}s")

    # Warmup
    print("\n[2/3] æ¨¡å‹é¢„çƒ­...")
    t0 = time.perf_counter()
    warmup_ppstructure()
    warmup_time = time.perf_counter() - t0
    print(f"  é¢„çƒ­è€—æ—¶: {warmup_time:.2f}s")

    # è¾“å‡ºç›®å½•
    output_dir = Path("test_output")
    output_dir.mkdir(exist_ok=True)

    # åˆ›å»ºå¤„ç†å™¨
    processor = ParallelPageProcessor(
        max_workers=int(os.getenv("EXAMPAPER_MAX_WORKERS", "4")),
        pipeline=pipeline,
    )
    processor.set_extraction_functions(
        extract_fn=extract_questions_from_page,
        save_fn=save_questions_for_page,
        is_valid_meta_fn=is_valid_meta,
    )

    # å¤„ç†è¿›åº¦å›è°ƒ
    def progress_cb(done: int, total: int, status: str, page: str):
        t_now = time.strftime("%H:%M:%S")
        print(f"  [{done}/{total}] {t_now} - {page}: {status}")

    # æ‰§è¡Œå¤„ç†
    print(f"\n[3/3] å¹¶è¡Œå¤„ç† {len(img_paths)} å¼ å›¾ç‰‡...")
    t0 = time.perf_counter()
    results = processor.process_pages_parallel(
        img_paths=img_paths,
        base_output_dir=output_dir,
        skip_existing=False,
        progress_callback=progress_cb,
        log=lambda m: print(f"  {m}"),
    )
    process_time = time.perf_counter() - t0

    # ç»Ÿè®¡ç»“æœ
    success = sum(1 for r in results if r.get("status") == "success")
    errors = sum(1 for r in results if r.get("status") == "error")
    total_questions = sum(r.get("question_count", 0) for r in results)

    print("\n" + "=" * 60)
    print("  æµ‹è¯•ç»“æœ")
    print("=" * 60)
    print(f"  å¤„ç†è€—æ—¶: {process_time:.2f}s")
    print(f"  å¹³å‡æ¯é¡µ: {process_time / len(img_paths):.2f}s")
    print(f"  ååé‡: {len(img_paths) / process_time:.2f} é¡µ/ç§’")
    print(f"  æˆåŠŸ: {success}, é”™è¯¯: {errors}")
    print(f"  æ£€æµ‹åˆ°é¢˜ç›®: {total_questions} é“")
    print("=" * 60)

    # åˆ†ææ€§èƒ½æ—¥å¿—
    print("\n[åˆ†æ] æ­£åœ¨åˆ†ææ€§èƒ½ç“¶é¢ˆ...")
    analyze_perf_trace()


def analyze_perf_trace():
    """åˆ†ææ€§èƒ½è¿½è¸ªæ—¥å¿—å¹¶ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
    trace_file = Path("perf_trace.jsonl")
    if not trace_file.exists():
        print("  âš ï¸ æ€§èƒ½è¿½è¸ªæ–‡ä»¶ä¸å­˜åœ¨")
        return

    # è¯»å–æ‰€æœ‰äº‹ä»¶
    events = []
    with trace_file.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    events.append(json.loads(line))
                except json.JSONDecodeError:
                    pass

    if not events:
        print("  âš ï¸ æœªæ‰¾åˆ°æ€§èƒ½äº‹ä»¶")
        return

    # æŒ‰é¡µé¢åˆ†ç»„åˆ†æ
    ocr_predict_events = [e for e in events if e.get("event") == "ocr.predict"]
    page_done_events = [e for e in events if e.get("event") == "page.done"]

    if not ocr_predict_events:
        print("  âš ï¸ æœªæ‰¾åˆ° ocr.predict äº‹ä»¶")
        return

    print(f"\n  âœ“ æ‰¾åˆ° {len(ocr_predict_events)} æ¡ ocr.predict è®°å½•")
    print(f"  âœ“ æ‰¾åˆ° {len(page_done_events)} æ¡ page.done è®°å½•")

    # ç»Ÿè®¡GPUé”ç­‰å¾…æ—¶é—´ vs çœŸå®æ¨ç†æ—¶é—´
    total_gpu_wait = sum(e.get("gpu_lock_wait_ms", 0) for e in ocr_predict_events)
    total_predict = sum(e.get("predict_ms", 0) for e in ocr_predict_events)
    avg_gpu_wait = total_gpu_wait / len(ocr_predict_events) if ocr_predict_events else 0
    avg_predict = total_predict / len(ocr_predict_events) if ocr_predict_events else 0

    print("\n  ã€GPUé” vs æ¨ç†æ—¶é—´ã€‘")
    print(f"    å¹³å‡ç­‰GPUé”: {avg_gpu_wait:.0f}ms")
    print(f"    å¹³å‡æ¨ç†æ—¶é—´: {avg_predict:.0f}ms")
    print(f"    ç­‰å¾…æ¯”ä¾‹: {100 * avg_gpu_wait / (avg_gpu_wait + avg_predict):.1f}%")

    # æ‰¾å‡ºæœ€æ…¢çš„3ä¸ªé¡µé¢
    page_timings = {}
    for e in page_done_events:
        page = e.get("page")
        total_ms = e.get("total_ms", 0)
        queue_wait_ms = e.get("queue_wait_ms", 0)
        if page:
            page_timings[page] = {"total_ms": total_ms, "queue_wait_ms": queue_wait_ms}

    # å…³è”OCRäº‹ä»¶
    for e in ocr_predict_events:
        page = e.get("page")
        if page and page in page_timings:
            page_timings[page]["gpu_wait_ms"] = e.get("gpu_lock_wait_ms", 0)
            page_timings[page]["predict_ms"] = e.get("predict_ms", 0)

    if page_timings:
        print("\n  ã€æœ€æ…¢çš„3ä¸ªé¡µé¢ã€‘")
        sorted_pages = sorted(page_timings.items(), key=lambda x: x[1].get("total_ms", 0), reverse=True)
        for page, timing in sorted_pages[:3]:
            print(f"\n    {page}:")
            print(f"      æ€»è€—æ—¶: {timing.get('total_ms', 0):.0f}ms")
            print(f"      é˜Ÿåˆ—ç­‰å¾…: {timing.get('queue_wait_ms', 0):.0f}ms")
            print(f"      GPUé”ç­‰å¾…: {timing.get('gpu_wait_ms', 0):.0f}ms")
            print(f"      æ¨ç†æ—¶é—´: {timing.get('predict_ms', 0):.0f}ms")

    # è¯Šæ–­ç»“è®º
    print("\n  ã€è¯Šæ–­ç»“è®ºã€‘")
    if avg_gpu_wait > avg_predict * 2:
        print("    âš ï¸ GPUé”ç­‰å¾…æ—¶é—´è¿‡é•¿ (æ˜¯æ¨ç†æ—¶é—´çš„2å€ä»¥ä¸Š)")
        print("    ğŸ’¡ å»ºè®®: å¢åŠ  EXAMPAPER_GPU_CONCURRENCY=2 (å¦‚æœæ˜¾å­˜å……è¶³)")
    elif avg_predict > 5000:
        print("    âš ï¸ æ¨ç†æ—¶é—´è¿‡é•¿ (>5ç§’/é¡µ)")
        print("    ğŸ’¡ å»ºè®®: æ£€æŸ¥GPUæ˜¯å¦æ­£å¸¸å·¥ä½œ (è¿è¡Œ nvidia-smi ç¡®è®¤)")
    else:
        print("    âœ“ GPUåˆ©ç”¨ç‡æ­£å¸¸")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—çš„é˜Ÿåˆ—ç­‰å¾…
    avg_queue_wait = sum(e.get("queue_wait_ms", 0) for e in page_done_events) / len(page_done_events) if page_done_events else 0
    if avg_queue_wait > 1000:
        print(f"    âš ï¸ é˜Ÿåˆ—ç­‰å¾…æ—¶é—´è¾ƒé•¿ (å¹³å‡{avg_queue_wait:.0f}ms)")
        print("    ğŸ’¡ å»ºè®®: å¢åŠ  EXAMPAPER_PREFETCH_SIZE æˆ–æ£€æŸ¥ç£ç›˜I/O")


if __name__ == "__main__":
    main()
